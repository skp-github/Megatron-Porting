{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install torch megatron-core"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jqIp_yEpwn09",
    "outputId": "48760fe2-8273-4052-dc92-9c2d16af649c"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Collecting megatron-core\n",
      "  Downloading megatron_core-0.13.1.tar.gz (743 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/743.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m743.6/743.6 kB\u001B[0m \u001B[31m45.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Collecting numpy<2.0.0 (from megatron-core)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.0/61.0 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.0/18.0 MB\u001B[0m \u001B[31m108.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: megatron-core\n",
      "  Building wheel for megatron-core (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for megatron-core: filename=megatron_core-0.13.1-cp312-cp312-linux_x86_64.whl size=2092872 sha256=636dd8d7f4cd1511940158225bfeb9efb7c091891cbb69bde3d52504c9c68e50\n",
      "  Stored in directory: /root/.cache/pip/wheels/a2/89/3a/e73801dac99baaeb15755bd738fab1a9143b2dd48434aeacba\n",
      "Successfully built megatron-core\n",
      "Installing collected packages: numpy, megatron-core\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed megatron-core-0.13.1 numpy-1.26.4\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       },
       "id": "8c3aae1de787473482731cd35d2a64b9"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tM7hAxx6tayB",
    "outputId": "9f347428-2dd7-4efb-d58e-a8c4d28e9996"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting gpt_oss_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpt_oss_trainer.py\n",
    "# gpt_oss_trainer.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Dict, Iterable, Optional, Tuple, Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from megatron.core import parallel_state\n",
    "from megatron.core import dist_checkpointing\n",
    "from megatron.core.pipeline_parallel.schedules import get_forward_backward_func\n",
    "from megatron.core.tensor_parallel.random import model_parallel_cuda_manual_seed\n",
    "from megatron.core.transformer.enums import AttnBackend\n",
    "from megatron.core.transformer.transformer_config import TransformerConfig\n",
    "from megatron.core.models.gpt.gpt_model import GPTModel\n",
    "from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec\n",
    "from megatron.core.datasets.utils import compile_helpers\n",
    "from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder\n",
    "from megatron.core.datasets.gpt_dataset import GPTDatasetConfig, MockGPTDataset\n",
    "from placeholder_tokenizer import _NullTokenizer\n",
    "\n",
    "_SEQUENCE_LENGTH = 4096\n",
    "_BASE_VOCAB_SIZE = 201_088\n",
    "\n",
    "\n",
    "class DistributedEnvironment:\n",
    "    @staticmethod\n",
    "    def initialize(\n",
    "        tensor_model_parallel_size: int = 1,\n",
    "        pipeline_model_parallel_size: int = 1,\n",
    "        expert_model_parallel_size: int = 1,\n",
    "    ) -> None:\n",
    "\n",
    "        parallel_state.destroy_model_parallel()\n",
    "\n",
    "\n",
    "        rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "        world_size = torch.cuda.device_count()\n",
    "        torch.cuda.set_device(rank)\n",
    "        torch.distributed.init_process_group(\n",
    "            backend=\"nccl\", world_size=world_size, rank=rank\n",
    "        )\n",
    "\n",
    "        # Megatron parallel init\n",
    "        parallel_state.initialize_model_parallel(\n",
    "            tensor_model_parallel_size=tensor_model_parallel_size,\n",
    "            pipeline_model_parallel_size=pipeline_model_parallel_size,\n",
    "            expert_model_parallel_size=expert_model_parallel_size,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def device() -> torch.device:\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class GPT20BStyleFactory:\n",
    "\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        sequence_length: int = _SEQUENCE_LENGTH,\n",
    "        vocab_size: int = _BASE_VOCAB_SIZE,\n",
    "        hidden_size: int = 2880,\n",
    "        num_layers: int = 24,\n",
    "        num_attention_heads: int = 64,\n",
    "        num_query_groups: int = 8,\n",
    "        kv_channels: int = 48,\n",
    "    ) -> GPTModel:\n",
    "        tp_size = parallel_state.get_tensor_model_parallel_world_size()\n",
    "        pp_size = parallel_state.get_pipeline_model_parallel_world_size()\n",
    "\n",
    "        config = TransformerConfig(\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            tensor_model_parallel_size=tp_size,\n",
    "            pipeline_model_parallel_size=pp_size,\n",
    "            num_query_groups=num_query_groups,\n",
    "            add_bias_linear=False,\n",
    "            normalization=\"RMSNorm\",\n",
    "            gated_linear_unit=True,\n",
    "            activation_func=F.silu,\n",
    "            num_moe_experts=32,\n",
    "            moe_ffn_hidden_size=4 * hidden_size,\n",
    "            moe_router_topk=4,\n",
    "            moe_aux_loss_coeff=1e-2,\n",
    "            attention_backend=AttnBackend.flash,\n",
    "            apply_residual_connection_post_layernorm=False,\n",
    "            use_cpu_initialization=True,\n",
    "            pipeline_dtype=torch.bfloat16,\n",
    "            kv_channels=kv_channels,\n",
    "        )\n",
    "\n",
    "        model = GPTModel(\n",
    "            config=config,\n",
    "            transformer_layer_spec=get_gpt_layer_local_spec(),\n",
    "            vocab_size=vocab_size,\n",
    "            max_sequence_length=sequence_length,\n",
    "            position_embedding_type=\"rope\",\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "class GPTMockDataModule:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length: int = _SEQUENCE_LENGTH,\n",
    "        vocab_size: int = _BASE_VOCAB_SIZE,\n",
    "        batch_size: int = 1,\n",
    "        seed: int = 0,\n",
    "        shuffle: bool = True,\n",
    "    ) -> None:\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # Compile CUDA helpers once per job (rank 0 guards)\n",
    "        if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "            if torch.distributed.get_rank() == 0:\n",
    "                compile_helpers()\n",
    "            torch.distributed.barrier()\n",
    "        else:\n",
    "            compile_helpers()\n",
    "\n",
    "        cfg = GPTDatasetConfig(\n",
    "            random_seed=self.seed,\n",
    "            sequence_length=self.sequence_length,\n",
    "            reset_position_ids=False,\n",
    "            reset_attention_mask=False,\n",
    "            eod_mask_loss=False,\n",
    "            tokenizer=_NullTokenizer(vocab_size=self.vocab_size),\n",
    "            mid_level_dataset_surplus=0.005,\n",
    "        )\n",
    "\n",
    "        datasets = BlendedMegatronDatasetBuilder(\n",
    "            MockGPTDataset, [1000, None, None], lambda: True, cfg\n",
    "        ).build()\n",
    "        self._train_ds = datasets[0]\n",
    "\n",
    "    def train_dataloader(self) -> Iterable[Dict[str, torch.Tensor]]:\n",
    "        return DataLoader(self._train_ds, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "\n",
    "\n",
    "\n",
    "class CausalLMForwardStep:\n",
    "\n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(\n",
    "        self, data_iterator: Iterable[Dict[str, torch.Tensor]], model: torch.nn.Module\n",
    "    ) -> Tuple[torch.Tensor, Callable]:\n",
    "        def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor):\n",
    "            losses = output_tensor.float()\n",
    "            loss_mask_f = loss_mask.view(-1).float()\n",
    "            loss = torch.sum(losses.view(-1) * loss_mask_f) / loss_mask_f.sum()\n",
    "            return loss, {\"lm loss\": loss}\n",
    "\n",
    "        batch = next(data_iterator)\n",
    "        tokens = batch[\"tokens\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        position_ids = batch[\"position_ids\"].to(self.device)\n",
    "        labels = batch[\"labels\"].to(self.device)\n",
    "        loss_mask = batch[\"loss_mask\"].to(self.device)\n",
    "\n",
    "        output_tensor = model(tokens, position_ids, attention_mask, labels=labels)\n",
    "        return output_tensor, partial(loss_func, loss_mask)\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    @staticmethod\n",
    "    def save(model: GPTModel, path: str) -> None:\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        sharded_sd = model.sharded_state_dict(prefix=\"\")\n",
    "        dist_checkpointing.save(sharded_state_dict=sharded_sd, checkpoint_dir=path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(model: GPTModel, path: str) -> GPTModel:\n",
    "        sharded_sd = model.sharded_state_dict(prefix=\"\")\n",
    "        ckpt = dist_checkpointing.load(sharded_state_dict=sharded_sd, checkpoint_dir=path)\n",
    "        model.load_state_dict(ckpt)\n",
    "        return model\n",
    "\n",
    "\n",
    "class MegatronTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: GPTModel,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        device: Optional[torch.device] = None,\n",
    "        num_microbatches: int = 1,\n",
    "        micro_batch_size: int = 2,\n",
    "        seq_length: int = _SEQUENCE_LENGTH,\n",
    "        decoder_seq_length: Optional[int] = None,\n",
    "        forward_only: bool = False,\n",
    "        log_interval: int = 1,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device or DistributedEnvironment.device()\n",
    "        self.num_microbatches = num_microbatches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.decoder_seq_length = decoder_seq_length or seq_length\n",
    "        self.forward_only = forward_only\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "        self.forward_backward = get_forward_backward_func()\n",
    "        self.forward_step = CausalLMForwardStep(self.device)\n",
    "\n",
    "        # Place model on device (important for non-pipeline stages)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def training_step(self, data_iter: Iterable[Dict[str, torch.Tensor]]) -> Dict[str, float]:\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        losses_reduced = self.forward_backward(\n",
    "            forward_step_func=self.forward_step,\n",
    "            data_iterator=data_iter,\n",
    "            model=self.model,\n",
    "            num_microbatches=self.num_microbatches,\n",
    "            seq_length=self.seq_length,\n",
    "            micro_batch_size=self.micro_batch_size,\n",
    "            decoder_seq_length=self.decoder_seq_length,\n",
    "            forward_only=self.forward_only,\n",
    "        )\n",
    "\n",
    "        if not self.forward_only:\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return losses_reduced\n",
    "\n",
    "    def fit(self, train_dataloader: Iterable, steps: int) -> None:\n",
    "        data_iter = iter(train_dataloader)\n",
    "        for step in range(1, steps + 1):\n",
    "            losses = self.training_step(data_iter)\n",
    "            if step % self.log_interval == 0 and (not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0):\n",
    "                print(f\"[step {step}] losses: {losses}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    DistributedEnvironment.initialize(\n",
    "        tensor_model_parallel_size=1,\n",
    "        pipeline_model_parallel_size=1,\n",
    "        expert_model_parallel_size=1,\n",
    "    )\n",
    "    model_parallel_cuda_manual_seed(123)\n",
    "\n",
    "    # Model\n",
    "    model = GPT20BStyleFactory.build()\n",
    "\n",
    "    # Data\n",
    "    dm = GPTMockDataModule(batch_size=1, shuffle=True)\n",
    "    dm.setup()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = Adam(\n",
    "        model.parameters(),\n",
    "        lr=2e-4,\n",
    "        betas=(0.9, 0.95),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = MegatronTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        num_microbatches=1,\n",
    "        micro_batch_size=2,\n",
    "        seq_length=_SEQUENCE_LENGTH,\n",
    "        forward_only=False,\n",
    "        log_interval=1,\n",
    "    )\n",
    "\n",
    "    # Train a few steps (sanity check)\n",
    "    trainer.fit(dm.train_dataloader(), steps=5)\n",
    "\n",
    "    ckpt_dir = os.path.join(os.getcwd(), \"ckpt\")\n",
    "    CheckpointManager.save(model, ckpt_dir)\n",
    "\n",
    "\n",
    "    model = CheckpointManager.load(model, ckpt_dir)\n",
    "    model.to(DistributedEnvironment.device())\n",
    "    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "        print(\"Successfully reloaded checkpoint.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! chmod +x run_trainer.sh\n",
    "! ./run_trainer.sh"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipXrHTvK5JSi",
    "outputId": "ed2132fd-f2e1-4276-ab3e-932c78088887"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "MEF1zUvx7yUw"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
