{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/NVIDIA/Megatron-LM.git\n",
    "%cd Megatron-LM/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jqIp_yEpwn09",
    "outputId": "48760fe2-8273-4052-dc92-9c2d16af649c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install regex\n",
    "!pip install pybind11\n",
    "!pip install --no-build-isolation -e ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tM7hAxx6tayB",
    "outputId": "9f347428-2dd7-4efb-d58e-a8c4d28e9996"
   },
   "outputs": [],
   "source": [
    "%%writefile gpt_oss_mock_data_trainer.py\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Dict, Iterable, Optional, Tuple, Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from megatron.core import parallel_state\n",
    "from megatron.core import dist_checkpointing\n",
    "from megatron.core.pipeline_parallel.schedules import get_forward_backward_func\n",
    "from megatron.core.tensor_parallel.random import model_parallel_cuda_manual_seed\n",
    "from megatron.core.transformer.enums import AttnBackend\n",
    "from megatron.core.transformer.transformer_config import TransformerConfig\n",
    "from megatron.core.models.gpt.gpt_model import GPTModel\n",
    "from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec\n",
    "from megatron.core.datasets.utils import compile_helpers\n",
    "from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder\n",
    "from megatron.core.datasets.gpt_dataset import GPTDatasetConfig, MockGPTDataset\n",
    "from megatron.training.tokenizer.tokenizer import _NullTokenizer\n",
    "\n",
    "_SEQUENCE_LENGTH = 4096\n",
    "_BASE_VOCAB_SIZE = 201_089\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Megatron-Core GPT Training\")\n",
    "    parser.add_argument(\"--tp\", type=int, default=1, help=\"Tensor parallel size\")\n",
    "    parser.add_argument(\"--pp\", type=int, default=1, help=\"Pipeline parallel size\")\n",
    "    parser.add_argument(\"--ep\", type=int, default=1, help=\"Expert parallel size\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "class DistributedEnvironment:\n",
    "    @staticmethod\n",
    "    def initialize(\n",
    "        tensor_model_parallel_size: int = 1,\n",
    "        pipeline_model_parallel_size: int = 1,\n",
    "        expert_model_parallel_size: int = 1,\n",
    "    ) -> None:\n",
    "\n",
    "        parallel_state.destroy_model_parallel()\n",
    "\n",
    "        rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "        world_size = torch.cuda.device_count()\n",
    "        torch.cuda.set_device(rank)\n",
    "        torch.distributed.init_process_group(\n",
    "            backend=\"nccl\", world_size=world_size, rank=rank\n",
    "        )\n",
    "\n",
    "        # Megatron parallel init\n",
    "        parallel_state.initialize_model_parallel(\n",
    "            tensor_model_parallel_size=tensor_model_parallel_size,\n",
    "            pipeline_model_parallel_size=pipeline_model_parallel_size,\n",
    "            expert_model_parallel_size=expert_model_parallel_size,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def device() -> torch.device:\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class GPT20BFactory:\n",
    "\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        sequence_length: int = _SEQUENCE_LENGTH,\n",
    "        vocab_size: int = _BASE_VOCAB_SIZE,\n",
    "        hidden_size: int = 2880,\n",
    "        num_layers: int = 24,\n",
    "        num_attention_heads: int = 64,\n",
    "        num_query_groups: int = 8,\n",
    "        kv_channels: int = 48,\n",
    "    ) -> GPTModel:\n",
    "        tp_size = parallel_state.get_tensor_model_parallel_world_size()\n",
    "        pp_size = parallel_state.get_pipeline_model_parallel_world_size()\n",
    "\n",
    "        config = TransformerConfig(\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            tensor_model_parallel_size=tp_size,\n",
    "            pipeline_model_parallel_size=pp_size,\n",
    "            num_query_groups=num_query_groups,\n",
    "            add_bias_linear=False,\n",
    "            normalization=\"RMSNorm\",\n",
    "            gated_linear_unit=True,\n",
    "            activation_func=F.silu,\n",
    "            num_moe_experts=32,\n",
    "            moe_ffn_hidden_size=4 * hidden_size,\n",
    "            moe_router_topk=4,\n",
    "            moe_aux_loss_coeff=1e-2,\n",
    "            attention_backend=AttnBackend.flash,\n",
    "            apply_residual_connection_post_layernorm=False,\n",
    "            use_cpu_initialization=True,\n",
    "            pipeline_dtype=torch.bfloat16,\n",
    "            kv_channels=kv_channels\n",
    "        )\n",
    "\n",
    "        model = GPTModel(\n",
    "            config=config,\n",
    "            transformer_layer_spec=get_gpt_layer_local_spec(),\n",
    "            vocab_size=vocab_size,\n",
    "            max_sequence_length=sequence_length,\n",
    "            position_embedding_type=\"rope\",\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "class GPTMockDataModule:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length: int = _SEQUENCE_LENGTH,\n",
    "        vocab_size: int = _BASE_VOCAB_SIZE,\n",
    "        batch_size: int = 1,\n",
    "        seed: int = 0,\n",
    "        shuffle: bool = True,\n",
    "    ) -> None:\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # Compile CUDA helpers once per job (rank 0 guards)\n",
    "        if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "            if torch.distributed.get_rank() == 0:\n",
    "                compile_helpers()\n",
    "            torch.distributed.barrier()\n",
    "        else:\n",
    "            compile_helpers()\n",
    "\n",
    "        cfg = GPTDatasetConfig(\n",
    "            random_seed=self.seed,\n",
    "            sequence_length=self.sequence_length,\n",
    "            reset_position_ids=False,\n",
    "            reset_attention_mask=False,\n",
    "            eod_mask_loss=False,\n",
    "            tokenizer=_NullTokenizer(vocab_size=self.vocab_size),\n",
    "            mid_level_dataset_surplus=0.005,\n",
    "        )\n",
    "\n",
    "        datasets = BlendedMegatronDatasetBuilder(\n",
    "            MockGPTDataset, [1000, None, None], lambda: True, cfg\n",
    "        ).build()\n",
    "        self._train_ds = datasets[0]\n",
    "\n",
    "    def train_dataloader(self) -> Iterable[Dict[str, torch.Tensor]]:\n",
    "        return DataLoader(self._train_ds, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "\n",
    "\n",
    "class CausalLMForwardStep:\n",
    "\n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(\n",
    "        self, data_iterator: Iterable[Dict[str, torch.Tensor]], model: torch.nn.Module\n",
    "    ) -> Tuple[torch.Tensor, Callable]:\n",
    "        def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor):\n",
    "            losses = output_tensor.float()\n",
    "            loss_mask_f = loss_mask.view(-1).float()\n",
    "            loss = torch.sum(losses.view(-1) * loss_mask_f) / loss_mask_f.sum()\n",
    "            return loss, {\"lm loss\": loss}\n",
    "\n",
    "        batch = next(data_iterator)\n",
    "        tokens = batch[\"tokens\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        position_ids = batch[\"position_ids\"].to(self.device)\n",
    "        labels = batch[\"labels\"].to(self.device)\n",
    "        loss_mask = batch[\"loss_mask\"].to(self.device)\n",
    "\n",
    "        output_tensor = model(tokens, position_ids, attention_mask, labels=labels)\n",
    "        return output_tensor, partial(loss_func, loss_mask)\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    @staticmethod\n",
    "    def save(model: GPTModel, path: str) -> None:\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        sharded_sd = model.sharded_state_dict(prefix=\"\")\n",
    "        dist_checkpointing.save(sharded_state_dict=sharded_sd, checkpoint_dir=path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(model: GPTModel, path: str) -> GPTModel:\n",
    "        sharded_sd = model.sharded_state_dict(prefix=\"\")\n",
    "        ckpt = dist_checkpointing.load(sharded_state_dict=sharded_sd, checkpoint_dir=path)\n",
    "        model.load_state_dict(ckpt)\n",
    "        return model\n",
    "\n",
    "\n",
    "class MegatronTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: GPTModel,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        device: Optional[torch.device] = None,\n",
    "        num_microbatches: int = 1,\n",
    "        micro_batch_size: int = 2,\n",
    "        seq_length: int = _SEQUENCE_LENGTH,\n",
    "        decoder_seq_length: Optional[int] = None,\n",
    "        forward_only: bool = False,\n",
    "        log_interval: int = 1,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device or DistributedEnvironment.device()\n",
    "        self.num_microbatches = num_microbatches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.decoder_seq_length = decoder_seq_length or seq_length\n",
    "        self.forward_only = forward_only\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "        self.forward_backward = get_forward_backward_func()\n",
    "        self.forward_step = CausalLMForwardStep(self.device)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def training_step(self, data_iter: Iterable[Dict[str, torch.Tensor]]) -> Dict[str, float]:\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        losses_reduced = self.forward_backward(\n",
    "            forward_step_func=self.forward_step,\n",
    "            data_iterator=data_iter,\n",
    "            model=self.model,\n",
    "            num_microbatches=self.num_microbatches,\n",
    "            seq_length=self.seq_length,\n",
    "            micro_batch_size=self.micro_batch_size,\n",
    "            decoder_seq_length=self.decoder_seq_length,\n",
    "            forward_only=self.forward_only,\n",
    "        )\n",
    "\n",
    "        if not self.forward_only:\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return losses_reduced\n",
    "\n",
    "    def fit(self, train_dataloader: Iterable, steps: int) -> None:\n",
    "        data_iter = iter(train_dataloader)\n",
    "        for step in range(1, steps + 1):\n",
    "            losses = self.training_step(data_iter)\n",
    "            if step % self.log_interval == 0 and (not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0):\n",
    "                print(f\"[step {step}] losses: {losses}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Print configuration on rank 0\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", \"0\")) == 0:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"TP={args.tp}, PP={args.pp}, EP={args.ep}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Initialize distributed environment\n",
    "    DistributedEnvironment.initialize(\n",
    "        tensor_model_parallel_size=args.tp,\n",
    "        pipeline_model_parallel_size=args.pp,\n",
    "        expert_model_parallel_size=args.ep,\n",
    "    )\n",
    "    model_parallel_cuda_manual_seed(123)\n",
    "\n",
    "    # Model\n",
    "    model = GPT20BFactory.build()\n",
    "\n",
    "    # Data\n",
    "    dm = GPTMockDataModule(batch_size=1, shuffle=True)\n",
    "    dm.setup()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = Adam(\n",
    "        model.parameters(),\n",
    "        lr=2e-4,\n",
    "        betas=(0.9, 0.95),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = MegatronTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        num_microbatches=1,\n",
    "        micro_batch_size=2,\n",
    "        seq_length=_SEQUENCE_LENGTH,\n",
    "        forward_only=False,\n",
    "        log_interval=1,\n",
    "    )\n",
    "\n",
    "    # Train a few steps (sanity check)\n",
    "    trainer.fit(dm.train_dataloader(), steps=5)\n",
    "\n",
    "    ckpt_dir = os.path.join(os.getcwd(), \"ckpt\")\n",
    "    CheckpointManager.save(model, ckpt_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! chmod +x run_trainer_mock_data.sh\n",
    "! ./run_trainer_mock_data.sh --ngpu 4 --tp 2 --pp 1 --ep 2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipXrHTvK5JSi",
    "outputId": "ed2132fd-f2e1-4276-ab3e-932c78088887"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
